{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch numpy==1.26 pandas scikit-learn plotly nltk transformers==4.46.3 sentence-transformers einops datasets gradio networkx umap-learn ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import display\n",
    "import umap\n",
    "from multiprocessing import Process\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV files to inspect their contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_df = pd.read_csv(os.path.join(current_dir, 'Program.csv'))\n",
    "program_df['title'] = program_df['title'].str.replace(r'<[^>]*>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_course_df = pd.read_csv(os.path.join(current_dir, 'ProgramCourse.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_type_df = pd.read_csv(os.path.join(current_dir, 'ProgramType.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df = pd.read_csv(os.path.join(current_dir, 'Course.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = sent_tokenize(text, language='french')\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove stopwords\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    remove_stopwords(text)\n",
    "    return text.strip()\n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MiniLM model and tokenizer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Function to Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "nltk.download('punkt')  # Ensure sentence tokenizer is available\n",
    "\n",
    "def generate_embeddings(\n",
    "    text_list, tokenizer, model, device=torch.device('cpu'), batch_size=32, pooling='mean', preprocess_fn=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using the specified pooling strategy.\n",
    "\n",
    "    Args:\n",
    "        text_list (list of str): List of input texts.\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer corresponding to the model.\n",
    "        model (transformers.AutoModel): Pre-trained language model.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        batch_size (int, optional): Number of texts to process per batch. Defaults to 32.\n",
    "        pooling (str, optional): Pooling strategy ('mean', 'max', 'concat'). Defaults to 'mean'.\n",
    "        preprocess_fn (callable, optional): Preprocessing function for text. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Generated embeddings.\n",
    "    \"\"\"\n",
    "    if pooling not in ['mean', 'max', 'concat']:\n",
    "        raise ValueError(f\"Unsupported pooling type '{pooling}'. Choose from 'mean', 'max', 'concat'.\")\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    embeddings = []\n",
    "\n",
    "    # Preprocess text if a function is provided\n",
    "    if preprocess_fn:\n",
    "        text_list = [preprocess_fn(text) for text in text_list]\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            batch_texts = text_list[i:i + batch_size]\n",
    "\n",
    "            # Tokenize each text into sentences if too long\n",
    "            tokenized_texts = [\n",
    "                sent_tokenize(text) if len(text.split()) > 512 else [text] for text in batch_texts\n",
    "            ]\n",
    "            \n",
    "            # Embed each sentence separately and aggregate\n",
    "            batch_embeddings = []\n",
    "            for sentences in tokenized_texts:\n",
    "                # Tokenize and process sentences\n",
    "                tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "                tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "                outputs = model(**tokens)\n",
    "\n",
    "                # Pool embeddings at sentence level\n",
    "                if pooling == 'mean':\n",
    "                    sentence_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "                elif pooling == 'max':\n",
    "                    sentence_embeddings = outputs.last_hidden_state.max(dim=1).values\n",
    "                elif pooling == 'concat':\n",
    "                    mean_pool = outputs.last_hidden_state.mean(dim=1)\n",
    "                    max_pool = outputs.last_hidden_state.max(dim=1).values\n",
    "                    sentence_embeddings = torch.cat((mean_pool, max_pool), dim=1)\n",
    "                \n",
    "                # Aggregate sentence embeddings\n",
    "                text_embedding = sentence_embeddings.mean(dim=0)\n",
    "                batch_embeddings.append(text_embedding)\n",
    "\n",
    "            # Append batch embeddings\n",
    "            embeddings.append(torch.stack(batch_embeddings).cpu())\n",
    "\n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = normalize(embeddings.numpy(), axis=1)\n",
    "\n",
    "    print(f\"Generated embeddings with '{pooling}' pooling. Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings for Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to include\n",
    "columns_to_include = [\n",
    "    'title', 'code', 'cycle', 'url', 'id'\n",
    "]\n",
    "\n",
    "# Ensure all columns are strings and handle NaN values\n",
    "for col in columns_to_include:\n",
    "    program_df[col] = program_df[col].astype(str).fillna('')\n",
    "\n",
    "# Concatenate the columns into a single string for each program\n",
    "program_texts = program_df[columns_to_include].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Generate embeddings for the combined texts\n",
    "program_embeddings = generate_embeddings(program_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Add embeddings to program_df\n",
    "program_df['vector'] = list(program_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca = PCA(n_components=2, random_state=42)\n",
    "program_pca_result = pca.fit_transform(program_embeddings)\n",
    "program_df['pca-one-program'] = program_pca_result[:, 0]\n",
    "program_df['pca-two-program'] = program_pca_result[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "program_tsne_result = tsne.fit_transform(program_embeddings)\n",
    "program_df['tsne-one-program'] = program_tsne_result[:, 0]\n",
    "program_df['tsne-two-program'] = program_tsne_result[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UMAP with desired parameters\n",
    "umap_projection = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "\n",
    "# Fit and transform the program embeddings\n",
    "program_umap_result = umap_projection.fit_transform(program_embeddings)\n",
    "\n",
    "# Add UMAP results to your DataFrame\n",
    "program_df['umap-one-program'] = program_umap_result[:, 0]\n",
    "program_df['umap-two-program'] = program_umap_result[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Generate Embeddings for Courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to include\n",
    "columns_to_include = [\n",
    "    'code', 'title', 'description', 'cycle', 'credits'\n",
    "]\n",
    "\n",
    "# Ensure all columns are strings and handle NaN values\n",
    "for col in columns_to_include:\n",
    "    course_df[col] = course_df[col].astype(str).fillna('')\n",
    "\n",
    "# Concatenate the columns into a single string for each program\n",
    "course_texts = course_df[columns_to_include].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Generate embeddings for the combined texts\n",
    "course_embeddings = generate_embeddings(course_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Add embeddings to program_df\n",
    "course_df['vector'] = list(course_embeddings)\n",
    "\n",
    "vectors = np.stack(course_df['vector'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Embeddings using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca = PCA(n_components=2, random_state=42)\n",
    "course_pca_result = pca.fit_transform(course_embeddings)\n",
    "course_df['pca-one-course'] = course_pca_result[:, 0]\n",
    "course_df['pca-two-course'] = course_pca_result[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Embeddings using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "course_tsne_result = tsne.fit_transform(course_embeddings)\n",
    "course_df['tsne-one-course'] = course_tsne_result[:, 0]\n",
    "course_df['tsne-two-course'] = course_tsne_result[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UMAP with desired parameters\n",
    "umap_projection = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "\n",
    "# Fit and transform the course embeddings\n",
    "course_umap_result = umap_projection.fit_transform(course_embeddings)\n",
    "\n",
    "# Add UMAP results to your DataFrame\n",
    "course_df['umap-one-course'] = course_umap_result[:, 0]\n",
    "course_df['umap-two-course'] = course_umap_result[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the Programs and Courses using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent data types for merge keys\n",
    "program_course_df['courseId'] = program_course_df['courseId'].astype(str)\n",
    "course_df['id'] = course_df['id'].astype(str)\n",
    "\n",
    "program_course_df['programId'] = program_course_df['programId'].astype(str)\n",
    "program_df['id'] = program_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames with suffixes to differentiate columns\n",
    "combined_df = pd.merge(\n",
    "    program_course_df,\n",
    "    program_df,\n",
    "    left_on='programId',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    suffixes=('_program_course', '_program')\n",
    ")\n",
    "\n",
    "# Print columns after first merge\n",
    "print(\"Columns after merging program_course_df and program_df:\")\n",
    "print(combined_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge with course_df\n",
    "combined_df = pd.merge(\n",
    "    combined_df,\n",
    "    course_df,\n",
    "    left_on='courseId',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    suffixes=('', '_course')\n",
    ")\n",
    "\n",
    "# Print columns after second merge\n",
    "print(\"Columns after merging with course_df:\")\n",
    "print(combined_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Rename columns for clarity\n",
    "combined_df.rename(columns={\n",
    "    'title': 'title_program',\n",
    "    'code': 'code_program',\n",
    "    'cycle': 'cycle_program',\n",
    "    'credits': 'credits_program',\n",
    "    'horaireCoursPdfJson': 'horaireCoursPdfJson_program',\n",
    "    'title_course': 'title_course',\n",
    "    'code_course': 'code_course',\n",
    "    'cycle_course': 'cycle_course',\n",
    "    'credits_course': 'credits_course',\n",
    "    'description': 'description_course'\n",
    "}, inplace=True)\n",
    "\n",
    "# Updated list of columns to include in the combined text\n",
    "columns_to_include = [\n",
    "    'programId', 'courseId', 'type',\n",
    "    'title_program', 'code_program', 'cycle_program', 'credits_program', 'horaireCoursPdfJson_program',\n",
    "    'title_course', 'code_course', 'cycle_course', 'credits_course', 'description_course'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns are strings and handle NaN values\n",
    "for col in columns_to_include:\n",
    "    combined_df[col] = combined_df[col].astype(str).fillna('')\n",
    "\n",
    "# Concatenate the columns into a single string for each record\n",
    "combined_texts = combined_df[columns_to_include].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Generate embeddings for the combined texts\n",
    "combined_embeddings = generate_embeddings(combined_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Add embeddings to combined_df\n",
    "combined_df['vector'] = list(combined_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca = PCA(n_components=2, random_state=42)\n",
    "combined_pca_result = pca.fit_transform(combined_embeddings)\n",
    "combined_df['pca-one-combined'] = combined_pca_result[:, 0]\n",
    "combined_df['pca-two-combined'] = combined_pca_result[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=50, max_iter=1000, random_state=42)\n",
    "combined_tsne_result = tsne.fit_transform(combined_embeddings)\n",
    "combined_df['tsne-one-combined'] = combined_tsne_result[:, 0]\n",
    "combined_df['tsne-two-combined'] = combined_tsne_result[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique programs\n",
    "num_unique_programs = combined_df['title_program'].nunique()\n",
    "print(f\"Number of unique programs: {num_unique_programs}\")\n",
    "\n",
    "# Initialize UMAP with desired parameters\n",
    "umap_projection = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "\n",
    "# Fit and transform the combined embeddings\n",
    "combined_umap_result = umap_projection.fit_transform(combined_embeddings)\n",
    "\n",
    "# Add UMAP results to the DataFrame\n",
    "combined_df['umap-one-combined'] = combined_umap_result[:, 0]\n",
    "combined_df['umap-two-combined'] = combined_umap_result[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_courses(user_completed_courses, user_liked_courses, program_id, combined_df, model_label, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend courses for a user based on completed and liked courses.\n",
    "\n",
    "    Args:\n",
    "        user_completed_courses (list): List of course IDs the user has completed.\n",
    "        user_liked_courses (list): List of course IDs the user likes or is interested in.\n",
    "        program_id (str): The program ID to restrict recommendations.\n",
    "        combined_df (pd.DataFrame): DataFrame containing course and program information.\n",
    "        model_label (str): The embedding model label to use for recommendations.\n",
    "        top_n (int): Number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        list: List of recommended courses with details.\n",
    "    \"\"\"\n",
    "    # Filter courses in the user's program\n",
    "    program_courses = combined_df[combined_df['programId'] == program_id]\n",
    "\n",
    "    # Extract embeddings for courses in the program\n",
    "    program_course_embeddings = np.stack(program_courses['vector'].values)\n",
    "    program_course_ids = program_courses['courseId'].tolist()\n",
    "    program_course_titles = program_courses['title_course'].tolist()\n",
    "\n",
    "    # Get embeddings for completed and liked courses\n",
    "    completed_embeddings = program_courses[program_courses['courseId'].isin(user_completed_courses)]['vector']\n",
    "    liked_embeddings = program_courses[program_courses['courseId'].isin(user_liked_courses)]['vector']\n",
    "\n",
    "    # Compute the user's profile embedding as the average of completed and liked embeddings\n",
    "    profile_embedding = np.mean(\n",
    "        np.concatenate([np.stack(completed_embeddings), np.stack(liked_embeddings)]), axis=0\n",
    "    )\n",
    "\n",
    "    # Compute similarity between profile embedding and program course embeddings\n",
    "    similarities = cosine_similarity(profile_embedding.reshape(1, -1), program_course_embeddings).flatten()\n",
    "\n",
    "    # Create a DataFrame to store recommendations\n",
    "    recommendation_df = pd.DataFrame({\n",
    "        'courseId': program_course_ids,\n",
    "        'title': program_course_titles,\n",
    "        'similarity': similarities\n",
    "    })\n",
    "\n",
    "    # Exclude completed courses\n",
    "    recommendation_df = recommendation_df[~recommendation_df['courseId'].isin(user_completed_courses)]\n",
    "\n",
    "    # Sort by similarity and select top N recommendations\n",
    "    recommendation_df = recommendation_df.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "\n",
    "    # Convert to list of dictionaries for output\n",
    "    recommendations = recommendation_df.to_dict('records')\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "user_completed = ['C101', 'C102', 'C103']  # Replace with actual completed course IDs\n",
    "user_liked = ['C104']  # Replace with actual liked course IDs\n",
    "program_id = 'P101'  # Replace with the user's program ID\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_courses(user_completed, user_liked, program_id, combined_df, model_label='MiniLM', top_n=5)\n",
    "\n",
    "# Print recommendations\n",
    "print(\"Recommended Courses:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"- {rec['title']} (Course ID: {rec['courseId']}, Similarity: {rec['similarity']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
